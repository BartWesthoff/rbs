{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu_sHuyKaSVr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from google.colab import files\n",
        "filename = \"data.xlsx\"\n",
        "df = pd.read_excel(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFIZlk1he-xd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools\n",
        "import statsmodels.tsa.api as tsa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "a1jyKQL6eOY7",
        "outputId": "18112fed-9d3a-45d1-d44f-700c9cce1636"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from google.colab import files\n",
        "import hashlib\n",
        "\n",
        "# Function to process the Excel file\n",
        "def process_excel(filepath):\n",
        "    \"\"\"\n",
        "    Load the dataset from the specified filepath, select certain columns,\n",
        "    anonymize the 'Resource' column, and ensure numeric columns are properly formatted.\n",
        "    Additionally, parse the 'Work Date' column into datetime.\n",
        "\n",
        "    Parameters:\n",
        "        filepath (str): The path to the Excel file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame containing the cleaned and anonymized data.\n",
        "    \"\"\"\n",
        "    # Columns to use\n",
        "    use_columns = [\n",
        "        'Resource', 'Work Date', 'Project Number', 'Activity Type',\n",
        "        'Company Number', 'Non Billable Hours', 'Billable Hours Worked',\n",
        "        'Department', 'Totaal uren'\n",
        "    ]\n",
        "\n",
        "    # Load the data\n",
        "    df = pd.read_excel(\n",
        "        filepath,\n",
        "        usecols=use_columns\n",
        "    )\n",
        "\n",
        "    # Convert 'Work Date' to datetime format\n",
        "    df['Work Date'] = pd.to_datetime(df['Work Date'], dayfirst=True)\n",
        "\n",
        "    # Convert numeric fields that use commas for decimals\n",
        "    df['Non Billable Hours'] = pd.to_numeric(df['Non Billable Hours'].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "    df['Billable Hours Worked'] = pd.to_numeric(df['Billable Hours Worked'].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "    df['Totaal uren'] = pd.to_numeric(df['Totaal uren'].astype(str).str.replace(',', '.'), errors='coerce')\n",
        "\n",
        "    # Anonymize the 'Resource' column using a hash function\n",
        "    def anonymize_resource(resource_name):\n",
        "        # Create a hash object with SHA-256\n",
        "        hash_obj = hashlib.sha256()\n",
        "        # Update the hash object with the resource name, encoded to bytes\n",
        "        hash_obj.update(resource_name.encode())\n",
        "        # Return the hexadecimal digest of the hash\n",
        "        return hash_obj.hexdigest()\n",
        "\n",
        "    # Apply anonymization to the 'Resource' column\n",
        "    df['Resource'] = df['Resource'].apply(anonymize_resource)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 1: Upload your Excel file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Process the uploaded Excel file\n",
        "filepath = next(iter(uploaded.keys()))\n",
        "df = process_excel(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "C3Nm45ilcaRq",
        "outputId": "95938acc-4066-4aea-8477-40cca8d217fa"
      },
      "outputs": [],
      "source": [
        "df.info()\n",
        "summary_statatistics = df.describe()\n",
        "\n",
        "summary_statatistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "lP0sWpFYeeo_",
        "outputId": "05bbc1f8-d3fa-4e2c-ed9c-98fc9290b0d3"
      },
      "outputs": [],
      "source": [
        "def summarize_categories(df):\n",
        "    \"\"\"\n",
        "    Summarize the DataFrame by categories, focusing on department and activity type.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the cleaned and anonymized data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with summarized categorical data.\n",
        "    \"\"\"\n",
        "    # Grouping by 'Department' and 'Activity Type' and aggregating data\n",
        "    summary = df.groupby(['Department', 'Activity Type']).agg({\n",
        "        'Billable Hours Worked': ['sum', 'mean'],\n",
        "        'Non Billable Hours': ['sum', 'mean'],\n",
        "        'Totaal uren': ['sum', 'mean']\n",
        "    }).reset_index()\n",
        "\n",
        "    # Renaming columns for clarity\n",
        "    summary.columns = ['Department', 'Activity Type', 'Total Billable Hours', 'Average Billable Hours',\n",
        "                       'Total Non Billable Hours', 'Average Non Billable Hours', 'Total Hours', 'Average Hours']\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "category_summary = summarize_categories(df)\n",
        "category_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "u9dcOTerejtz",
        "outputId": "02e99637-f5b1-47df-e25b-e2486390b817"
      },
      "outputs": [],
      "source": [
        "# Filter out rows where 'Department' is either 'DevOps' or 'Sales Telecom'\n",
        "df = df[~df['Department'].isin(['DevOps', 'Sales Telecom'])]\n",
        "\n",
        "\"\"\"\n",
        "Explanation:\n",
        "    df['Department'].isin(['DevOps', 'Sales Telecom']) creates a boolean mask where entries are True if the department is 'DevOps' or 'Sales Telecom'.\n",
        "\n",
        "    The ~ operator negates the boolean mask, so it selects rows that do not match these departments.\n",
        "\n",
        "    This filtered DataFrame, df_filtered, will contain only the rows where the department is neither 'DevOps' nor 'Sales Telecom'.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "G6J17lV-enBX",
        "outputId": "f2e737ca-f1ea-4f18-caea-c9a76e0b83d6"
      },
      "outputs": [],
      "source": [
        "category_summary = summarize_categories(df)\n",
        "category_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqivIlQoeqkT",
        "outputId": "69472394-fac8-48b7-bac0-b82facbcd4b3"
      },
      "outputs": [],
      "source": [
        "# Step 1: Copy\n",
        "df_perday = df\n",
        "\n",
        "# Step 2: Transform the data per day by summing up hours\n",
        "daily_summary = df_perday.groupby('Work Date').agg({\n",
        "    'Billable Hours Worked': 'sum',\n",
        "    'Non Billable Hours': 'sum',\n",
        "    'Totaal uren': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "daily_summary['Billable Hours Worked'] = daily_summary['Billable Hours Worked'].round(2)\n",
        "daily_summary['Non Billable Hours'] = daily_summary['Non Billable Hours'].round(2)\n",
        "daily_summary['Totaal uren'] = daily_summary['Totaal uren'].round(2)\n",
        "\n",
        "print(daily_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "ag4uoD66etZx",
        "outputId": "9d052781-e222-4c19-9d55-db9b369d0610"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert 'Work Date' to datetime\n",
        "df_perday['Work Date'] = pd.to_datetime(df_perday['Work Date'])\n",
        "\n",
        "# Group by both 'Work Date' and 'Activity Type', then sum 'Totaal uren'\n",
        "grouped_data = df_perday.groupby([pd.Grouper(key='Work Date', freq='M'), 'Activity Type'])['Totaal uren'].sum().unstack()\n",
        "\n",
        "# Compute total hours across all activities per month\n",
        "grouped_data['Total Monthly Hours'] = grouped_data.sum(axis=1)\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot each activity type if available in the data\n",
        "if 'Ticket' in grouped_data.columns:\n",
        "    plt.plot(grouped_data.index, grouped_data['Ticket'], label='Ticket Times', marker='o', linestyle='-', color='blue')\n",
        "if 'Project Task' in grouped_data.columns:\n",
        "    plt.plot(grouped_data.index, grouped_data['Project Task'], label='Project Times', marker='o', linestyle='-', color='green')\n",
        "if 'Regular Time' in grouped_data.columns:\n",
        "    plt.plot(grouped_data.index, grouped_data['Regular Time'], label='Regular Time Entries', marker='o', linestyle='-', color='red')\n",
        "\n",
        "# Plot total monthly hours\n",
        "plt.plot(grouped_data.index, grouped_data['Total Monthly Hours'], label='Total Monthly Hours', marker='o', linestyle='-', color='black')\n",
        "\n",
        "plt.title('Monthly Work Hours Trends by Activity Type')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Calculate variance and mean deviation (bias)\n",
        "variance = grouped_data['Total Monthly Hours'].var()\n",
        "mean_hours = grouped_data['Total Monthly Hours'].mean()\n",
        "bias = (grouped_data['Total Monthly Hours'] - mean_hours).mean()\n",
        "\n",
        "# Annotate variance and bias on the plot\n",
        "plt.annotate(f'Variance: {variance:.2f}', xy=(0.05, 0.95), xycoords='axes fraction', fontsize=10, backgroundcolor='white')\n",
        "plt.annotate(f'Mean Bias: {bias:.2f}', xy=(0.05, 0.90), xycoords='axes fraction', fontsize=10, backgroundcolor='white')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Zsyh0sRWexUf",
        "outputId": "d3f502a9-3d7f-45e5-ef79-e7850466a711"
      },
      "outputs": [],
      "source": [
        "# Assuming df_perday is loaded with 'Work Date', 'Department', 'Activity Type', and 'Totaal uren'\n",
        "# Filter for Tickets in IT and Telecom\n",
        "tickets = df_perday[(df_perday['Activity Type'] == 'Ticket') &\n",
        "                    df_perday['Department'].isin(['Operations & Service IT', 'Operations & Service Telecom'])]\n",
        "\n",
        "# Group by Month and Department\n",
        "monthly_tickets = tickets.groupby([pd.Grouper(key='Work Date', freq='M'), 'Department'])['Totaal uren'].sum().unstack().fillna(0)\n",
        "\n",
        "# Plotting\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(14, 7))\n",
        "monthly_tickets.plot(kind='line', marker='o', linestyle='-')\n",
        "plt.title('Monthly Ticket Hours: IT vs. Telecom')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Ticket Hours')\n",
        "plt.legend(title='Department')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm4xE1xrPpCp"
      },
      "source": [
        "Voorspelling voor Operations & Service IT projecten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wrwm83OoMEi",
        "outputId": "0517874b-05af-43a8-e4f1-88b668094b97"
      },
      "outputs": [],
      "source": [
        "# Filter for 'Operations & Service IT' department and 'Project Task' activity type\n",
        "filtered_df = df[(df['Department'] == 'Operations & Service IT') & (df['Activity Type'] == 'Project Task')]\n",
        "\n",
        "# Group by 'Work Date' and sum 'Totaal uren'\n",
        "daily_summary = filtered_df.groupby('Work Date').agg({\n",
        "    'Totaal uren': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Set 'Work Date' as the index\n",
        "daily_data = daily_summary.set_index('Work Date')['Totaal uren']\n",
        "daily_data\n",
        "filtered_df = df[(df['Department'] == 'Operations & Service IT') & (df['Activity Type'] == 'Project Task')]\n",
        "\n",
        "# Group by month and sum 'Totaal uren'\n",
        "monthly_summary = filtered_df.groupby(pd.Grouper(key='Work Date', freq='M')).agg({\n",
        "    'Totaal uren': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Set 'Work Date' as the index\n",
        "monthly_data = monthly_summary.set_index('Work Date')['Totaal uren']\n",
        "monthly_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bQ5rXrz_KEC0",
        "outputId": "8f785a6a-3a67-4827-ad28-f75254e66ed5"
      },
      "outputs": [],
      "source": [
        "#Base model\n",
        "# Persistence Model\n",
        "months_to_forecast = 12  # Forecasting for the next 12 months\n",
        "X = monthly_data.values\n",
        "train, test = X[:-months_to_forecast], X[-months_to_forecast:]\n",
        "predictions = np.roll(test, 1)\n",
        "predictions[0] = train[-1]  # Set the first prediction to the last value of the train set\n",
        "\n",
        "# Calculate RMSE for the Persistence Model\n",
        "persistence_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmE9_B_Fq8SA",
        "outputId": "f8246226-aae4-452f-bec4-225aa5c70495"
      },
      "outputs": [],
      "source": [
        "# Step 2: Check for Stationarity\n",
        "def test_stationarity(timeseries):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    result = adfuller(timeseries)\n",
        "    dfoutput = pd.Series(result[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in result[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(monthly_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBtsiy7R5Hqg"
      },
      "source": [
        "Data is niet stationare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "ze0jEH6Ep2bM",
        "outputId": "aec117d3-0b5e-4ca8-c50e-8e853bf19240"
      },
      "outputs": [],
      "source": [
        "# Step 3: Plot ACF and PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(monthly_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(monthly_data, lags=10, ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "LZEUeJ4msU6Z",
        "outputId": "71e73ea8-e77e-440c-ace1-7d97fbf1989b"
      },
      "outputs": [],
      "source": [
        "# Step 4: Decompose the Time Series\n",
        "decomposition = sm.tsa.seasonal_decompose(monthly_data, model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n92dtBv5SbG"
      },
      "source": [
        "Laat duidelijke seasonal trend zien"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "FL9tWVFivbV_",
        "outputId": "411c8c2e-6e37-4976-fa1f-d471b4e38b17"
      },
      "outputs": [],
      "source": [
        "differenced_data = monthly_data.diff().dropna()\n",
        "test_stationarity(differenced_data)\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(differenced_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(differenced_data, lags=10, ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASzKhYgu5cIU"
      },
      "source": [
        "Na differencing is de data is getransformeerd naar een stationaire reeks, wel met verminderde/verdwenen autocorrelatie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "9yvwEjjdvirb",
        "outputId": "416a4860-b138-4409-8321-0572f160b9ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Prepare the data for linear regression\n",
        "time = np.arange(len(monthly_data)).reshape(-1, 1)\n",
        "values = monthly_data.values.reshape(-1, 1)\n",
        "\n",
        "# Fit linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(time, values)\n",
        "trend = model.predict(time)\n",
        "\n",
        "# Detrend the data\n",
        "detrended_data = monthly_data - trend.flatten()\n",
        "\n",
        "# Check stationarity of detrended data\n",
        "test_stationarity(detrended_data)\n",
        "\n",
        "# Re-plot ACF and PACF for detrended data\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(detrended_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(detrended_data, lags=10, ax=ax[1])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Tcbl-h-hJ2"
      },
      "source": [
        "Differencing is prefered over linear detrending for making the data stationar.\n",
        "When differencing the ACF and PACF plots show significant autocorrelation at lag 1, and the values taper off quickly after that. This indicates that differencing has removed much of the autocorrelation in the data. However,given the drop at lag 6 in both the ACF and PACF plots, it's possible that some seasonal pattern remains in the data even after differencing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "NtpQMtPDEnZ7",
        "outputId": "9ce5e912-04e8-4fa5-89d3-bcb50f2528e5"
      },
      "outputs": [],
      "source": [
        "#SARIMA\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "tst_size = 23\n",
        "trn, tst = monthly_data[:-tst_size], monthly_data[-tst_size:]\n",
        "best_aic=float('inf')\n",
        "best_bic=float('inf')\n",
        "aic_settings=()\n",
        "bic_settings=()\n",
        "\n",
        "for p,d,q in itertools.product([ 1, 2, 3],[1, 2],[1, 2]):\n",
        "    model = sm.tsa.SARIMAX(trn, order=(p, d, q))\n",
        "    try:\n",
        "        model_fit = model.fit()\n",
        "        aic = model_fit.aic\n",
        "        bic = model_fit.bic\n",
        "\n",
        "        if aic < best_aic:\n",
        "            best_aic = aic\n",
        "            aic_settings = (p, d, q)\n",
        "\n",
        "        if bic < best_bic:\n",
        "            best_bic = bic\n",
        "            bic_settings = (p, d, q)\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "print(\"Best AIC:\", best_aic)\n",
        "print(\"AIC Settings:\", aic_settings)\n",
        "print(\"Best BIC:\", best_bic)\n",
        "print(\"BIC Settings:\", bic_settings)\n",
        "\n",
        "# Performance\n",
        "model = tsa.ARIMA(trn, order=(23, 2, 1))\n",
        "model_fit = model.fit()\n",
        "prd = model_fit.predict(start=len(trn), end=len(trn)+len(tst)-1)\n",
        "plt.plot(tst,'.-')\n",
        "plt.plot(prd, '--',color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "t0VDBEWsEQdr",
        "outputId": "3dd5b422-c12c-4e5d-9b9d-65869f59d546"
      },
      "outputs": [],
      "source": [
        "# SARIMA met validatie set\n",
        "tst_size = 12  # Number of test periods\n",
        "val_size = 12  # Number of validation periods\n",
        "\n",
        "trn = monthly_data[:-tst_size - val_size]\n",
        "val = monthly_data[-tst_size - val_size:-tst_size]\n",
        "tst = monthly_data[-tst_size:]\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_settings = ()\n",
        "\n",
        "# Grid search for SARIMA parameters\n",
        "for p, d, q in itertools.product(range(0, 3), range(0, 3), range(0, 3)):\n",
        "    for P, D, Q, m in itertools.product(range(0, 3), range(0, 3), range(0, 3), [12]):\n",
        "        try:\n",
        "            model = SARIMAX(trn, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
        "            model_fit = model.fit(disp=False)\n",
        "            val_predictions = model_fit.get_forecast(len(val)).predicted_mean\n",
        "            rmse = np.sqrt(mean_squared_error(val, val_predictions))\n",
        "\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_settings = ((p, d, q), (P, D, Q, m))\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(\"Best settings:\", best_settings)\n",
        "\n",
        "# Performance on the test set\n",
        "model = SARIMAX(monthly_data, order=best_settings[0], seasonal_order=best_settings[1])\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Forecasting\n",
        "forecast_steps = len(tst)\n",
        "prd = model_fit.get_forecast(steps=forecast_steps).predicted_mean\n",
        "forecast_dates = pd.date_range(start=tst.index[0], periods=forecast_steps, freq='M')\n",
        "forecast_df = pd.DataFrame({'Forecast': prd.values}, index=forecast_dates)\n",
        "\n",
        "# RMSE for the test set\n",
        "test_rmse = np.sqrt(mean_squared_error(tst, prd))\n",
        "print(f'Test RMSE: {test_rmse:.3f}')\n",
        "\n",
        "# Plotting the actual vs forecasted values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_data.index, monthly_data, label='Actual')\n",
        "plt.plot(forecast_df.index, forecast_df['Forecast'], label='Forecast', linestyle='--', color='red')\n",
        "plt.title('SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik5qJuLohbzL"
      },
      "source": [
        "**Subconclusion:**when choosing the appropriate aggregation level for time series analysis, it's important to balance business needs and the robustness of the model. For strategic purposes, where the goal is to identify long-term trends and seasonality, monthly aggregation is typically sufficient and commonly used. However, due to the limited number of data points currently available in the dataset and low accurcy when plotting for a monthly scope, weekly aggregation is advised. Weekly aggregation strikes a balance between granularity and noise reduction, providing more data points for meaningful analysis while capturing important patterns and trends. This approach enhances the accuracy of the forecasts and aligns with both business requirements and machine learning practices. As more data points become available, it can be reconsidered transitioning to monthly aggregation for strategic analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baSRjT7dgdpV",
        "outputId": "98a1db1c-1790-40e6-9e5c-efae264f21cd"
      },
      "outputs": [],
      "source": [
        "# Aggregate de data bij week, summing 'Totaal uren'\n",
        "df = pd.DataFrame(df)\n",
        "\n",
        "df['Work Date'] = pd.to_datetime(df['Work Date'])\n",
        "\n",
        "print(df.dtypes)\n",
        "\n",
        "weekly_summary = df.resample('W-Sun', on='Work Date').sum().reset_index().sort_values('Work Date')\n",
        "weekly_summary.head()\n",
        "\n",
        "weekly_data = weekly_summary.set_index('Work Date')['Totaal uren']\n",
        "print(weekly_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EjmkS_VB-A1f",
        "outputId": "3bb9b42c-ebb5-4905-eb71-6cf359a51932"
      },
      "outputs": [],
      "source": [
        "# Predictie scope is week\n",
        "weekly_summary = filtered_df.groupby(pd.Grouper(key='Work Date', freq='W')).agg({'Totaal uren': 'sum'}).reset_index()\n",
        "weekly_data = weekly_summary.set_index('Work Date')['Totaal uren']\n",
        "\n",
        "# Step 1: Check for Stationarity\n",
        "def test_stationarity(timeseries):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    result = adfuller(timeseries)\n",
        "    dfoutput = pd.Series(result[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in result[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(weekly_data)\n",
        "\n",
        "# Step 2: Plot ACF and PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(weekly_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(weekly_data, lags=10, ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Decompose the Time Series\n",
        "decomposition = sm.tsa.seasonal_decompose(weekly_data, model='additive')\n",
        "fig = decomposition.plot()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Difference the data and re-check for stationarity\n",
        "differenced_data = weekly_data.diff().dropna()\n",
        "test_stationarity(differenced_data)\n",
        "\n",
        "# Step 5: Plot ACF and PACF for differenced data\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(differenced_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(differenced_data, lags=10, ax=ax[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrAfHlpvOes9"
      },
      "source": [
        "Deze analyse van de dataset op week scope laat zien dat, na het differentiëren, stationariteit is bereikt en significante autocorrelaties aanwezig blijven. Dit zijn positieve signalen die erop wijzen dat tijdreeksmodellen effectief onderliggende patronen in de gegevens zou kunnen vastleggen en voorspellen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF5_iDIEEfnI",
        "outputId": "cf0c507e-c214-4913-c67e-2906f4c08c98"
      },
      "outputs": [],
      "source": [
        "weekly_data.describe()\n",
        "weekly_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IIMklp00NAGB",
        "outputId": "82665e90-f286-41d9-a98b-f0318f4a0510"
      },
      "outputs": [],
      "source": [
        "# Prediction scope op week\n",
        "# Step 1: Check for Stationarity\n",
        "def test_stationarity(timeseries):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    result = adfuller(timeseries)\n",
        "    dfoutput = pd.Series(result[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in result[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(weekly_data)\n",
        "\n",
        "# Step 2: Plot ACF and PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(weekly_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(weekly_data, lags=10, ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Decompose the Time Series\n",
        "decomposition = sm.tsa.seasonal_decompose(weekly_data, model='additive', period=52)  # Assuming 52 weeks seasonality\n",
        "fig = decomposition.plot()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Difference the data and re-check for stationarity\n",
        "differenced_data = weekly_data.diff().dropna()\n",
        "test_stationarity(differenced_data)\n",
        "\n",
        "# Step 5: Plot ACF and PACF for differenced data\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(differenced_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(differenced_data, lags=10, ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Persistence Model\n",
        "weeks_to_forecast = 12  # Forecasting for the next 12 weeks\n",
        "X = weekly_data.values\n",
        "train, test = X[:-weeks_to_forecast], X[-weeks_to_forecast:]\n",
        "predictions = np.roll(test, 1)\n",
        "predictions[0] = train[-1]  # Set the first prediction to the last value of the train set\n",
        "\n",
        "# Create a DataFrame for the persistence model\n",
        "forecast_dates = weekly_data.index[-weeks_to_forecast:]\n",
        "persistence_forecast = pd.DataFrame({'Actual': weekly_data[-weeks_to_forecast:], 'Persistence': predictions}, index=forecast_dates)\n",
        "\n",
        "# Calculate RMSE for the Persistence Model\n",
        "persistence_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "\n",
        "# Step 6: Identify the best SARIMA model parameters using grid search\n",
        "p = d = q = range(0, 3)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in pdq]  # Using 52 for yearly seasonality in weekly data\n",
        "\n",
        "best_aic = float(\"inf\")\n",
        "best_pdq = None\n",
        "best_seasonal_pdq = None\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            mod = SARIMAX(weekly_data, order=param, seasonal_order=param_seasonal)\n",
        "            results = mod.fit(disp=False)\n",
        "            if results.aic < best_aic:\n",
        "                best_aic = results.aic\n",
        "                best_pdq = param\n",
        "                best_seasonal_pdq = param_seasonal\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"Best SARIMA order: {best_pdq}\")\n",
        "print(f\"Best seasonal order: {best_seasonal_pdq}\")\n",
        "\n",
        "# Step 7: Fit the SARIMA model with parameters\n",
        "mod = SARIMAX(weekly_data, order=best_pdq, seasonal_order=best_seasonal_pdq)\n",
        "results = mod.fit(disp=False)\n",
        "\n",
        "# Step 8: Forecast future values\n",
        "forecast_steps = weeks_to_forecast\n",
        "forecast = results.get_forecast(steps=forecast_steps)\n",
        "sarima_pred = forecast.predicted_mean\n",
        "sarima_pred.index = forecast_dates  # Ensure the SARIMA predictions have the same index\n",
        "\n",
        "# Create a DataFrame for the SARIMA model\n",
        "sarima_forecast = pd.DataFrame({'SARIMA': sarima_pred}, index=forecast_dates)\n",
        "\n",
        "# Calculate RMSE for the SARIMA Model\n",
        "sarima_rmse = np.sqrt(mean_squared_error(test, sarima_pred))\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')\n",
        "\n",
        "# Combine the actual values, persistence model predictions, and SARIMA predictions\n",
        "combined_forecast = pd.concat([persistence_forecast, sarima_forecast], axis=1)\n",
        "\n",
        "# Step 9: Plot\n",
        "combined_forecast.plot(figsize=(12, 6))\n",
        "plt.title('Actual vs Persistence Model vs SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Evaluate the model performance on the test set (already done above)\n",
        "# Persistence RMSE\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "# SARIMA RMSE\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3gkz6kbFUX3"
      },
      "source": [
        "ACF en PACF plots (na en voor  differencing) bevstigen dat de dataset een sterke tijd component heeft, wat tijd serie analyse bruikbaar maakt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5gllBqkP_YP"
      },
      "source": [
        "# **Voorspellen totaal uren projecten en tasks \tOperations & Service Telecom en \tOperations & Service IT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lvrrM3VgP4HS",
        "outputId": "af51d1be-1d6c-4328-cf81-d526a86bc50c"
      },
      "outputs": [],
      "source": [
        "#Week scope\n",
        "# Step 1: Check for Stationarity\n",
        "def test_stationarity(timeseries):\n",
        "    print('Results of Dickey-Fuller Test:')\n",
        "    result = adfuller(timeseries)\n",
        "    dfoutput = pd.Series(result[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
        "    for key, value in result[4].items():\n",
        "        dfoutput['Critical Value (%s)' % key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "test_stationarity(weekly_data)\n",
        "\n",
        "# Step 2: Plot ACF and PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(weekly_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(weekly_data, lags=10, ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Decompose the Time Series\n",
        "decomposition = sm.tsa.seasonal_decompose(weekly_data, model='additive', period=52)  # Assuming 52 weeks seasonality\n",
        "fig = decomposition.plot()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Difference the data and re-check for stationarity\n",
        "differenced_data = weekly_data.diff().dropna()\n",
        "test_stationarity(differenced_data)\n",
        "\n",
        "# Step 5: Plot ACF and PACF for differenced data\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "sm.graphics.tsa.plot_acf(differenced_data, lags=10, ax=ax[0])\n",
        "sm.graphics.tsa.plot_pacf(differenced_data, lags=10, ax=ax[1])\n",
        "plt.show()\n",
        "\n",
        "# Persistence Model\n",
        "weeks_to_forecast = 26  # Forecasting for the next 26 weeks (approximately 6 months)\n",
        "X = weekly_data.values\n",
        "train, test = X[:-weeks_to_forecast], X[-weeks_to_forecast:]\n",
        "predictions = np.roll(test, 1)\n",
        "predictions[0] = train[-1]  # Set the first prediction to the last value of the train set\n",
        "\n",
        "# Create a DataFrame for the persistence model\n",
        "forecast_dates = weekly_data.index[-weeks_to_forecast:]\n",
        "persistence_forecast = pd.DataFrame({'Actual': weekly_data[-weeks_to_forecast:], 'Persistence': predictions}, index=forecast_dates)\n",
        "\n",
        "# Calculate RMSE for the Persistence Model\n",
        "persistence_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "total_weeks = len(weekly_data)\n",
        "train_size = int(0.7 * total_weeks)\n",
        "val_size = int(0.15 * total_weeks)\n",
        "test_size = total_weeks - train_size - val_size\n",
        "\n",
        "trn = weekly_data[:train_size]\n",
        "val = weekly_data[train_size:train_size + val_size]\n",
        "tst = weekly_data[train_size + val_size:]\n",
        "\n",
        "best_rmse = float('inf')\n",
        "best_settings = ()\n",
        "\n",
        "# Step 6: Grid search for SARIMA parameters using validation set\n",
        "p = d = q = range(0, 3)\n",
        "pdq = list(itertools.product(p, d, q))\n",
        "seasonal_pdq = [(x[0], x[1], x[2], 52) for x in pdq]  # Using 52 for yearly seasonality in weekly data\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in seasonal_pdq:\n",
        "        try:\n",
        "            model = SARIMAX(trn, order=param, seasonal_order=param_seasonal)\n",
        "            model_fit = model.fit(disp=False)\n",
        "            val_predictions = model_fit.get_forecast(steps=len(val)).predicted_mean\n",
        "            rmse = np.sqrt(mean_squared_error(val, val_predictions))\n",
        "\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_settings = (param, param_seasonal)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"Best settings: {best_settings}\")\n",
        "\n",
        "# Step 7: Fit the SARIMA model with  parameters\n",
        "model = SARIMAX(weekly_data[:train_size + val_size], order=best_settings[0], seasonal_order=best_settings[1])\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Step 8: Forecast future values\n",
        "forecast_steps = test_size\n",
        "forecast = model_fit.get_forecast(steps=forecast_steps)\n",
        "sarima_pred = forecast.predicted_mean\n",
        "sarima_pred.index = weekly_data.index[train_size + val_size:train_size + val_size + forecast_steps]\n",
        "\n",
        "sarima_forecast = pd.DataFrame({'SARIMA': sarima_pred}, index=sarima_pred.index)\n",
        "\n",
        "# Calculate RMSE for the SARIMA Model\n",
        "sarima_rmse = np.sqrt(mean_squared_error(tst, sarima_pred))\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')\n",
        "\n",
        "# Actual values, persistence model predictions, and SARIMA predictions\n",
        "combined_forecast = pd.concat([persistence_forecast, sarima_forecast], axis=1)\n",
        "\n",
        "# Step 9: Plot\n",
        "combined_forecast.plot(figsize=(12, 6))\n",
        "plt.title('Actual vs Persistence Model vs SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 10: Evaluate the model performance on the test set (already done above)\n",
        "# Persistence RMSE\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "# SARIMA RMSE\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COlR9UDoLocM"
      },
      "source": [
        "Ook voor \tOperations & Service Telecom en \tOperations & Service IT tickets en projecten dataset, laat een sterke stijgende trend over de jaren zien en een seasonal effect. Na het differentiëren, stationariteit is bereikt en significante autocorrelaties aanwezig blijven en dus een tijdcomponent. Dit zijn positieve signalen die erop wijzen dat tijdreeksmodellen effectief onderliggende patronen in de gegevens zou kunnen vastleggen en voorspellen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "635p8j6_HaZ1"
      },
      "outputs": [],
      "source": [
        "# SARIMA Model\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "best_aic = float('inf')\n",
        "best_bic = float('inf')\n",
        "aic_settings = ()\n",
        "bic_settings = ()\n",
        "\n",
        "# Grid search for SARIMA parameters\n",
        "for p, d, q in itertools.product([2, 3, 4, 6, 7, 8], [1, 2], [1, 2]):\n",
        "    for P, D, Q, m in itertools.product(range(0, 3), range(0, 3), range(0, 3), [52]):\n",
        "        try:\n",
        "            model = SARIMAX(trn, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
        "            model_fit = model.fit(disp=False)\n",
        "            aic = model_fit.aic\n",
        "            bic = model_fit.bic\n",
        "\n",
        "            if aic < best_aic:\n",
        "                best_aic = aic\n",
        "                aic_settings = (p, d, q, P, D, Q, m)\n",
        "\n",
        "            if bic < best_bic:\n",
        "                best_bic = bic\n",
        "                bic_settings = (p, d, q, P, D, Q, m)\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(\"Best AIC:\", best_aic)\n",
        "print(\"AIC Settings:\", aic_settings)\n",
        "print(\"Best BIC:\", best_bic)\n",
        "print(\"BIC Settings:\", bic_settings)\n",
        "\n",
        "# Fit the SARIMA model with parameters\n",
        "p, d, q, P, D, Q, m = aic_settings\n",
        "model = SARIMAX(weekly_data[:train_size + val_size], order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Forecast future values\n",
        "forecast_steps = test_size\n",
        "forecast = model_fit.get_forecast(steps=forecast_steps)\n",
        "sarima_pred = forecast.predicted_mean\n",
        "sarima_pred.index = weekly_data.index[train_size + val_size:train_size + val_size + forecast_steps]\n",
        "\n",
        "#RMSE, MAE, and MAPE for the SARIMA Model\n",
        "sarima_rmse = np.sqrt(mean_squared_error(tst, sarima_pred))\n",
        "sarima_mae = mean_absolute_error(tst, sarima_pred)\n",
        "sarima_mape = mean_absolute_percentage_error(tst, sarima_pred)\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')\n",
        "print(f'SARIMA Model Test MAE: {sarima_mae:.3f}')\n",
        "print(f'SARIMA Model Test MAPE: {sarima_mape:.3f}')\n",
        "\n",
        "combined_forecast = pd.concat([forecast_df, pd.DataFrame({'Prophet': prophet_pred}, index=prophet_pred.index), pd.DataFrame({'SARIMA': sarima_pred}, index=sarima_pred.index)], axis=1)\n",
        "\n",
        "combined_forecast.plot(figsize=(12, 6))\n",
        "plt.title('Actual vs Persistence Model vs Prophet Model vs SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEwnrg0SZsEy",
        "outputId": "72b90b19-566b-467b-ea26-816403d0eeee"
      },
      "outputs": [],
      "source": [
        "df['Work Date'] = pd.to_datetime(df['Work Date'])\n",
        "daily_summary = df.groupby('Work Date').agg({'Totaal uren': 'sum'}).reset_index()\n",
        "daily_summary = daily_summary[daily_summary['Totaal uren'] >= 50]\n",
        "daily_data = daily_summary.set_index('Work Date')['Totaal uren']\n",
        "daily_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LqnBpz1UcbWb",
        "outputId": "3ed7c40d-9d51-45d6-9103-a625ba8d0600"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import itertools\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Training, validation, and test sets\n",
        "total_days = len(daily_data)\n",
        "train_size = int(0.7 * total_days)\n",
        "val_size = int(0.15 * total_days)\n",
        "test_size = total_days - train_size - val_size\n",
        "\n",
        "trn = daily_data[:train_size]\n",
        "val = daily_data[train_size:train_size + val_size]\n",
        "tst = daily_data[train_size + val_size:]\n",
        "\n",
        "# Persistence Model\n",
        "days_to_forecast = test_size\n",
        "X = daily_data.values\n",
        "train, test = X[:-days_to_forecast], X[-days_to_forecast:]\n",
        "predictions = np.roll(test, 1)\n",
        "predictions[0] = train[-1]  # Set the first prediction to the last value of the train set\n",
        "\n",
        "forecast_dates = daily_data.index[-days_to_forecast:]\n",
        "persistence_forecast = pd.DataFrame({'Actual': daily_data[-days_to_forecast:], 'Persistence': predictions}, index=forecast_dates)\n",
        "\n",
        "#RMSE for the Persistence Model\n",
        "persistence_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "\n",
        "# Grid search for SARIMA parameters\n",
        "best_rmse = float('inf')\n",
        "best_settings = ()\n",
        "\n",
        "for p, d, q in itertools.product(range(0, 3), range(0, 3), range(0, 3)):\n",
        "    for P, D, Q, m in itertools.product(range(0, 3), range(0, 3), range(0, 3), [12]):\n",
        "        try:\n",
        "            model = SARIMAX(trn, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
        "            model_fit = model.fit(disp=False)\n",
        "            val_predictions = model_fit.get_forecast(len(val)).predicted_mean\n",
        "            rmse = np.sqrt(mean_squared_error(val, val_predictions))\n",
        "\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_settings = ((p, d, q), (P, D, Q, m))\n",
        "\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(\"Best settings:\", best_settings)\n",
        "\n",
        "# Fit the SARIMA model with the identified parameters\n",
        "model = SARIMAX(daily_data[:train_size + val_size], order=best_settings[0], seasonal_order=best_settings[1])\n",
        "model_fit = model.fit(disp=False)\n",
        "\n",
        "# Forecast future values\n",
        "forecast_steps = test_size\n",
        "forecast = model_fit.get_forecast(steps=forecast_steps)\n",
        "sarima_pred = forecast.predicted_mean\n",
        "sarima_pred.index = daily_data.index[train_size + val_size:train_size + val_size + forecast_steps]\n",
        "\n",
        "# Calculate Forecast Errors\n",
        "sarima_errors = tst - sarima_pred\n",
        "\n",
        "# Calculate RMSE and MFE for Bias\n",
        "sarima_rmse = np.sqrt(mean_squared_error(tst, sarima_pred))\n",
        "sarima_mfe = np.mean(sarima_errors)\n",
        "print(f'SARIMA Model Test RMSE: {sarima_rmse:.3f}')\n",
        "print(f'SARIMA Model Mean Forecast Error (Bias): {sarima_mfe:.3f}')\n",
        "\n",
        "# Combine the actual values, persistence model predictions, and SARIMA predictions into one DataFrame\n",
        "combined_forecast = pd.concat([persistence_forecast, pd.DataFrame({'SARIMA': sarima_pred}, index=sarima_pred.index)], axis=1)\n",
        "\n",
        "# Plot the combined forecast DataFrame\n",
        "combined_forecast.plot(figsize=(12, 6))\n",
        "plt.title('Actual vs Persistence Model vs SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Forecast Errors\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(sarima_errors, label='SARIMA Forecast Errors')\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.title('SARIMA Model Forecast Errors')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Actual vs Forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(tst, label='Actual')\n",
        "plt.plot(sarima_pred, label='SARIMA Forecast')\n",
        "plt.title('Actual vs SARIMA Model Forecast')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enAbkGcbGS1"
      },
      "source": [
        "De lagere RMSE van het persistence model suggereert dat de total hours een zekere mate van continuïteit en lage variabiliteit vertonen, waardoor eenvoudige voorspellingen op basis van recente gegevens effectief zijn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjfySd557jCD"
      },
      "source": [
        "Prophet modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qROy-5Lxo52W",
        "outputId": "360df040-a81f-423d-b7f1-f6574d234a39"
      },
      "outputs": [],
      "source": [
        "#Week scope\n",
        "!pip install Prophet\n",
        "from prophet import Prophet\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Training, validation, and test sets\n",
        "total_weeks = len(weekly_data)\n",
        "train_size = int(0.7 * total_weeks)\n",
        "val_size = int(0.15 * total_weeks)\n",
        "test_size = total_weeks - train_size - val_size\n",
        "\n",
        "trn = weekly_data[:train_size]\n",
        "val = weekly_data[train_size:train_size + val_size]\n",
        "tst = weekly_data[train_size + val_size:]\n",
        "\n",
        "# Prophet Model\n",
        "prophet_df = weekly_summary.rename(columns={'Work Date': 'ds', 'Totaal uren': 'y'})\n",
        "prophet_train = prophet_df[:train_size + val_size]\n",
        "\n",
        "model_prophet = Prophet()\n",
        "model_prophet.fit(prophet_train)\n",
        "future = model_prophet.make_future_dataframe(periods=test_size, freq='W')\n",
        "forecast_prophet = model_prophet.predict(future)\n",
        "\n",
        "# Extract the test period forecast\n",
        "prophet_pred = forecast_prophet.set_index('ds')['yhat'].iloc[train_size + val_size:]\n",
        "\n",
        "# Calculate RMSE for the Prophet Model\n",
        "prophet_rmse = np.sqrt(mean_squared_error(tst, prophet_pred))\n",
        "print(f'Prophet Model Test RMSE: {prophet_rmse:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "8OE93fh0uuR3",
        "outputId": "e42e83ca-9752-40a7-f3a9-2c217e00a053"
      },
      "outputs": [],
      "source": [
        "combined_forecast = pd.DataFrame({'Actual': tst, 'Prophet': prophet_pred}, index=tst.index)\n",
        "\n",
        "# Plot\n",
        "combined_forecast.plot(figsize=(12, 6))\n",
        "plt.title('Actual vs Prophet Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAZVwW1EC6Ip",
        "outputId": "0f270ac2-8a88-40b6-a873-bfd1a9a1af45"
      },
      "outputs": [],
      "source": [
        "prophet_errors = tst - prophet_pred\n",
        "\n",
        "# Calculate Mean Forecast Error (MFE) for Bias\n",
        "prophet_mfe = np.mean(prophet_errors)\n",
        "print(f'Prophet Model Mean Forecast Error (Bias): {prophet_mfe:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9rcIMw1-Nmm",
        "outputId": "9e069fba-2d90-4e62-b75d-7925286b5a17"
      },
      "outputs": [],
      "source": [
        "total_weeks = len(weekly_data)\n",
        "train_size = int(0.7 * total_weeks)\n",
        "val_size = int(0.15 * total_weeks)\n",
        "test_size = total_weeks - train_size - val_size\n",
        "\n",
        "trn = weekly_data[:train_size]\n",
        "val = weekly_data[train_size:train_size + val_size]\n",
        "tst = weekly_data[train_size + val_size:]\n",
        "test_weekly_average = weekly_data.loc[tst.index].mean()\n",
        "normalized_weekly_rmse = prophet_rmse / test_weekly_average\n",
        "\n",
        "# Print the normalized weekly RMSE\n",
        "print(f'Normalized Weekly RMSE: {normalized_weekly_rmse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IMu19UBH38s",
        "outputId": "ce988703-b74c-4040-86fc-1b4ff8354eb0"
      },
      "outputs": [],
      "source": [
        "rmse = 101.545\n",
        "n = len(tst)\n",
        "\n",
        "# Calculate the Standard Error\n",
        "se = rmse / np.sqrt(n)\n",
        "\n",
        "# Confidence interval multiplier for 95% CI\n",
        "z_score = 1.96\n",
        "\n",
        "# Calculate the Margin of Error\n",
        "moe = z_score * se\n",
        "rmse_percentage = (rmse / test_weekly_average) * 100\n",
        "print(f'Average Daily Total Hours: {test_weekly_average:.2f}')\n",
        "print(f'RMSE as Percentage of Average Daily Total Hours: {rmse_percentage:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "79HTI5zq1c9D",
        "outputId": "4acca7f9-d2ce-41a6-da31-6447eb51aa62"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Persistence Model\n",
        "weeks_to_forecast = test_size\n",
        "X = weekly_data.values\n",
        "train, test = X[:-weeks_to_forecast], X[-weeks_to_forecast:]\n",
        "predictions = np.roll(test, 1)\n",
        "predictions[0] = train[-1]  # Set the first prediction to the last value of the train set\n",
        "\n",
        "#RMSE for the Persistence Model\n",
        "persistence_rmse = np.sqrt(mean_squared_error(test, predictions))\n",
        "print(f'Persistence Model Test RMSE: {persistence_rmse:.3f}')\n",
        "\n",
        "forecast_dates = weekly_data.index[-weeks_to_forecast:]\n",
        "forecast_df = pd.DataFrame({'Actual': test, 'Prediction': predictions}, index=forecast_dates)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(weekly_data.index, weekly_data, label='Actual')\n",
        "plt.plot(forecast_df.index, forecast_df['Prediction'], label='Persistence Forecast', linestyle='--')\n",
        "plt.title('Actual vs Persistence Model Forecast (Weekly)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "prophet_df = weekly_summary.rename(columns={'Work Date': 'ds', 'Totaal uren': 'y'})\n",
        "prophet_train = prophet_df[:train_size + val_size]\n",
        "\n",
        "model_prophet = Prophet()\n",
        "model_prophet.fit(prophet_train)\n",
        "future = model_prophet.make_future_dataframe(periods=test_size, freq='W')\n",
        "forecast_prophet = model_prophet.predict(future)\n",
        "\n",
        "prophet_pred = forecast_prophet.set_index('ds')['yhat'].iloc[train_size + val_size:]\n",
        "\n",
        "#RMSE for the Prophet Model\n",
        "prophet_rmse = np.sqrt(mean_squared_error(tst, prophet_pred))\n",
        "print(f'Prophet Model Test RMSE: {prophet_rmse:.3f}')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(weekly_data.index, weekly_data, label='Actual')\n",
        "plt.plot(forecast_df.index, forecast_df['Prediction'], label='Persistence Forecast', linestyle='--')\n",
        "plt.plot(prophet_pred.index, prophet_pred, label='Prophet Forecast', linestyle='--')\n",
        "plt.title('Actual vs Prophet Model vs Persistence Model Forecast (Weekly)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vl7kpEL78E_U",
        "outputId": "4e60a3ec-3db0-4bc1-a9bd-8d2138c801f1"
      },
      "outputs": [],
      "source": [
        "#Op dag\n",
        "!pip install Prophet\n",
        "from prophet import Prophet\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Aggregate the data by day, summing 'Totaal uren'\n",
        "daily_summary = df.groupby('Work Date').agg({'Totaal uren': 'sum'}).reset_index()\n",
        "print(daily_summary)\n",
        "\n",
        "# Filter out days with less than 50 total hours\n",
        "daily_summary = daily_summary[daily_summary['Totaal uren'] >= 50]\n",
        "\n",
        "# Set 'Work Date' as the index\n",
        "daily_data = daily_summary.set_index('Work Date')['Totaal uren']\n",
        "print(daily_data.head())\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "total_days = len(daily_data)\n",
        "train_size = int(0.7 * total_days)\n",
        "val_size = int(0.15 * total_days)\n",
        "test_size = total_days - train_size - val_size\n",
        "\n",
        "trn = daily_data[:train_size]\n",
        "val = daily_data[train_size:train_size + val_size]\n",
        "tst = daily_data[train_size + val_size:]\n",
        "\n",
        "# Prophet Model\n",
        "prophet_df = daily_summary.rename(columns={'Work Date': 'ds', 'Totaal uren': 'y'})\n",
        "prophet_train = prophet_df[:train_size + val_size]\n",
        "\n",
        "model_prophet = Prophet()\n",
        "model_prophet.fit(prophet_train)\n",
        "\n",
        "# Extend the future dataframe to 365 days for a full year prediction\n",
        "future = model_prophet.make_future_dataframe(periods=1095, freq='D')\n",
        "forecast_prophet = model_prophet.predict(future)\n",
        "\n",
        "# Extract the test period forecast\n",
        "prophet_pred = forecast_prophet.set_index('ds')['yhat'].iloc[train_size + val_size:total_days + 1095]\n",
        "\n",
        "# Calculate RMSE for the Prophet Model on the test set\n",
        "prophet_rmse = np.sqrt(mean_squared_error(tst, prophet_pred[:test_size]))\n",
        "print(f'Prophet Model Test RMSE: {prophet_rmse:.3f}')\n",
        "\n",
        "# Combine the actual values and predictions into one DataFrame\n",
        "combined_forecast = pd.DataFrame({'Actual': tst, 'Prophet': prophet_pred[:test_size]}, index=tst.index)\n",
        "\n",
        "# Plot the actual vs forecasted data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(combined_forecast.index, combined_forecast['Actual'], label='Actual')\n",
        "plt.plot(combined_forecast.index, combined_forecast['Prophet'], label='Prophet Forecast')\n",
        "plt.title('Actual vs Prophet Model')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the future forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(daily_data.index, daily_data, label='Historical Data')\n",
        "plt.plot(forecast_prophet['ds'], forecast_prophet['yhat'], label='Prophet Forecast')\n",
        "plt.title('Prophet Forecast for Next Year')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Hours')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vl9eXSmmtnp",
        "outputId": "54c77cb8-39f0-44b5-c030-c8c3d07282fd"
      },
      "outputs": [],
      "source": [
        "daily_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU--CBYaYy_1",
        "outputId": "ba1caa39-f753-4294-abe9-fd96df9520b7"
      },
      "outputs": [],
      "source": [
        "prophet_errors = tst - prophet_pred\n",
        "\n",
        "# Calculate Mean Forecast Error (MFE) for Bias\n",
        "prophet_mfe = np.mean(prophet_errors)\n",
        "print(f'Prophet Model Mean Forecast Error (Bias): {prophet_mfe:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RVmFCtZZOhU"
      },
      "source": [
        "Negative bais dus tendency to overpredict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qZYSaGeZQe_"
      },
      "source": [
        "Prophet heeft hogere accurcy dan SARIMA. Prophet heeft de neiging te overpredictne en SARIMA te onderpredicten. Op dit moment functioneer het base model nog steeds het best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkcafzKy8q_h",
        "outputId": "1d856bcd-7d3f-4b94-963b-7f506f1c5807"
      },
      "outputs": [],
      "source": [
        "total_days = len(daily_data)\n",
        "train_size = int(0.7 * total_days)\n",
        "val_size = int(0.15 * total_days)\n",
        "test_size = total_days - train_size - val_size\n",
        "\n",
        "trn = daily_data[:train_size]\n",
        "val = daily_data[train_size:train_size + val_size]\n",
        "tst = daily_data[train_size + val_size:]\n",
        "test_daily_average = daily_data.loc[tst.index].mean()\n",
        "\n",
        "# Normalize the RMSE with the daily average\n",
        "normalized_daily_rmse = prophet_rmse / test_daily_average\n",
        "\n",
        "# Print the normalized daily RMSE\n",
        "print(f'Normalized Daily RMSE: {normalized_daily_rmse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEGTmaDZGkOV",
        "outputId": "7f16d77f-106b-40c9-f091-e486ef54c1db"
      },
      "outputs": [],
      "source": [
        "test_daily_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV86qtoN-6rZ",
        "outputId": "24f294c4-0c18-4d81-b979-64851bbeb1d0"
      },
      "outputs": [],
      "source": [
        "rmse = 31.579\n",
        "n = len(tst)\n",
        "\n",
        "# Calculate the Standard Error\n",
        "se = rmse / np.sqrt(n)\n",
        "\n",
        "# Confidence interval multiplier for 95% CI\n",
        "z_score = 1.96\n",
        "\n",
        "# Calculate the Margin of Error\n",
        "moe = z_score * se\n",
        "rmse_percentage = (rmse / test_daily_average) * 100\n",
        "print(f'Average Daily Total Hours: {test_daily_average:.2f}')\n",
        "print(f'RMSE as Percentage of Average Daily Total Hours: {rmse_percentage:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
