{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEmqVaYRJ2zv"
      },
      "source": [
        "# Install Requirements\n",
        "\n",
        "Use:\n",
        "```bash\n",
        "pip install pdfplumber python-docx sentence-transformers langchain_huggingface langchain_community transformers numpy pandas tqdm chromadb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVKTtF7hJ6el"
      },
      "source": [
        "# Make dataset from documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from docx import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_files_recursively(directory: str, extensions: list) -> list:\n",
        "    \"\"\"\n",
        "    Recursively find all files in the given directory with specified extensions.\n",
        "    Args:\n",
        "        directory (str): Directory to search.\n",
        "        extensions (list): List of file extensions (e.g., ['.pdf', '.docx']).\n",
        "    Returns:\n",
        "        List of file paths matching the extensions.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if any(filename.lower().endswith(ext) for ext in extensions):\n",
        "                files.append(os.path.join(root, filename))\n",
        "    return files\n",
        "\n",
        "def extract_pdf_text(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from a PDF file using pdfplumber.\n",
        "    Args:\n",
        "        file_path (str): Path to the PDF file.\n",
        "    Returns:\n",
        "        Extracted text, or empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            text = \"\\n\".join(page.extract_text() for page in pdf.pages if page.extract_text())\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_docx_text(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract text from a DOCX file using python-docx.\n",
        "    Args:\n",
        "        file_path (str): Path to the DOCX file.\n",
        "    Returns:\n",
        "        Extracted text, or empty string if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        text = \"\\n\".join(para.text for para in doc.paragraphs)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def read_text_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Read content from a TXT or MD file.\n",
        "    Args:\n",
        "        file_path (str): Path to the TXT or MD file.\n",
        "    Returns:\n",
        "        File content, or empty string if reading fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def convert_file(file_path: str, output_format: str = \"txt\") -> str:\n",
        "    \"\"\"\n",
        "    Convert a single file to the specified format using simpler tools.\n",
        "    Args:\n",
        "        file_path (str): Path to the file to convert.\n",
        "        output_format (str): Desired output format ('txt' for plain text).\n",
        "    Returns:\n",
        "        Extracted content, or empty string if conversion fails.\n",
        "    \"\"\"\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext == \".pdf\":\n",
        "        return extract_pdf_text(file_path)\n",
        "    elif ext == \".docx\":\n",
        "        return extract_docx_text(file_path)\n",
        "    elif ext in [\".txt\", \".md\"]:\n",
        "        return read_text_file(file_path)\n",
        "    else:\n",
        "        print(f\"Unsupported file extension for {file_path}\")\n",
        "        return \"\"\n",
        "\n",
        "def process_files(data_dir: str, output_pickle: str, extensions: list, output_format: str = \"txt\"):\n",
        "    \"\"\"\n",
        "    Process all files in the data directory, extract text, and save to a pickled DataFrame.\n",
        "    Args:\n",
        "        data_dir (str): Directory containing the files to process.\n",
        "        output_pickle (str): Path to the output pickled DataFrame file.\n",
        "        extensions (list): List of file extensions to process (e.g., ['.pdf', '.docx', '.txt', '.md']).\n",
        "        output_format (str): Output format ('txt' for plain text).\n",
        "    \"\"\"\n",
        "    # Find all files recursively\n",
        "    files = find_files_recursively(data_dir, extensions)\n",
        "    print(f\"Found {len(files)} files to process.\")\n",
        "\n",
        "    # Process files with a progress bar\n",
        "    data = []\n",
        "    for file in tqdm(files):\n",
        "        content = convert_file(file, output_format)\n",
        "        if content:\n",
        "            data.append({\"filename\": os.path.basename(file), \"content\": content})\n",
        "\n",
        "    # Save to pickled DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    df['content'] = df['content'].str.strip()\n",
        "    df['content'] = df['content'].str.replace(r'\\n+', ' ', regex=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehaH9dMHggy7",
        "outputId": "fa53303c-fa02-4657-b281-7f03cf513a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 30 files to process.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [00:01<00:00, 15.74it/s]\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "data_directory = \"files\"  # Adjust to your data folder\n",
        "output_pickle_file = os.path.join(data_directory, \"processed_files.pkl\")\n",
        "file_extensions = [\".pdf\", \".docx\", \".txt\", \".md\"]  # Supported extensions\n",
        "\n",
        "# Run the processing\n",
        "df = process_files(data_directory, output_pickle_file, file_extensions, output_format=\"txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabJvjzvJ9cF"
      },
      "source": [
        "### Check the created dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyHUCiUmhSz3",
        "outputId": "4597daa1-559b-4787-ee1b-e5cd37da0aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            filename  \\\n",
            "0  Uitstel beantwoording vragen van de leden Vedd...   \n",
            "1                                   Sprekerslijst.md   \n",
            "2  De uitsluiting van het Afghaanse vrouwen voetb...   \n",
            "3  Het bericht dat de gemeente Haarlem ouders vra...   \n",
            "4  Het beëindigen van de VWS-subsidie aan het IK...   \n",
            "\n",
            "                                             content  \n",
            "0  2 Tweede Kamer der Staten-Generaal Vergaderjaa...  \n",
            "1  # Tweede Kamer ## DER STATEN-GENERAAL ### SPRE...  \n",
            "2  2 Tweede Kamer der Staten-Generaal Vergaderjaa...  \n",
            "3  2 Tweede Kamer der Staten-Generaal Vergaderjaa...  \n",
            "4  2025Z06621 (ingezonden 7 april 2025) Vragen va...  \n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI5IOe8cKQyu"
      },
      "source": [
        "# Run the Vector store and RAG part (ChromaDB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from transformers import pipeline\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_text(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> list:\n",
        "    \"\"\"\n",
        "    Split the text into chunks for embedding.\n",
        "    Args:\n",
        "        text (str): The text to split.\n",
        "        chunk_size (int): Size of each chunk in characters.\n",
        "        chunk_overlap (int): Overlap between chunks in characters.\n",
        "    Returns:\n",
        "        list: List of text chunks.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def create_vector_store(df: pd.DataFrame, embeddings: str, persist_directory: str = \"./chroma_db\") -> Chroma:\n",
        "    \"\"\"\n",
        "    Create a ChromaDB vector store from text chunks with metadata.\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing the document content and filenames.\n",
        "        embeddings (str): The embeddings model name (e.g., \"sentence-transformers/all-MiniLM-L6-v2\").\n",
        "        persist_directory (str): Directory to persist the ChromaDB store.\n",
        "    Returns:\n",
        "        Chroma: The Chroma vector store instance.\n",
        "    \"\"\"\n",
        "    # Create the embeddings model\n",
        "    embeddings_model = HuggingFaceEmbeddings(model_name=embeddings)\n",
        "\n",
        "    # Split text into chunks per document and collect metadata\n",
        "    chunks = []\n",
        "    metadatas = []\n",
        "    for _, row in df.iterrows():\n",
        "        doc_chunks = split_text(row[\"content\"])\n",
        "        chunks.extend(doc_chunks)\n",
        "        metadatas.extend([{\"filename\": row[\"filename\"]} for _ in doc_chunks])\n",
        "\n",
        "    # Create Chroma vector store from texts and metadata\n",
        "    vectorstore = Chroma.from_texts(\n",
        "        texts=chunks,\n",
        "        embedding=embeddings_model,\n",
        "        metadatas=metadatas,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    print(f\"ChromaDB vector store created and persisted to {persist_directory}.\")\n",
        "    return vectorstore\n",
        "\n",
        "def setup_rag_chain(vectorstore) -> RetrievalQA:\n",
        "    \"\"\"\n",
        "    Set up the RAG chain with a ChromaDB-based retriever.\n",
        "    Args:\n",
        "        vectorstore (Chroma): The Chroma vector store instance.\n",
        "    Returns:\n",
        "        RetrievalQA: The configured RAG chain.\n",
        "    \"\"\"\n",
        "    # Set up the retriever from the Chroma vector store\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    # Load a small local LLM\n",
        "    llm_pipeline = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\", device=\"cpu\")\n",
        "    llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "    # Create the RAG chain\n",
        "    rag_chain = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",  # Simple concatenation of retrieved docs\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True  # Return the source for educational purposes\n",
        "    )\n",
        "    return rag_chain\n",
        "\n",
        "def run_example_query(rag_chain: RetrievalQA, query: str):\n",
        "    \"\"\"\n",
        "    Run an example query on the RAG system.\n",
        "    Args:\n",
        "        rag_chain (RetrievalQA): The RAG chain.\n",
        "        query (str): The query to ask.\n",
        "    \"\"\"\n",
        "    result = rag_chain.invoke({\"query\": query})\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Answer: {result['result']}\")\n",
        "    print(\"Source Documents:\")\n",
        "    for doc in result['source_documents']:\n",
        "        print(f\"- {doc.page_content[:100]}... (from {doc.metadata['filename']})\")\n",
        "€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VNhNVp7I5qW",
        "outputId": "2caf552d-974a-48a3-c722-35acadcdcf90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChromaDB vector store created and persisted to ./chroma_db.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG system set up.\n"
          ]
        }
      ],
      "source": [
        "# Create the vector store using ChromaDB\n",
        "embeddings = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "vectorstore = create_vector_store(df, embeddings, persist_directory=\"./chroma_db\")\n",
        "\n",
        "# Set up the RAG chain\n",
        "rag_chain = setup_rag_chain(vectorstore)\n",
        "print(\"RAG system set up.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buqesUBHKdHT"
      },
      "source": [
        "### Run test querys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvzagRVOGtdl",
        "outputId": "242445fa-5720-4a20-d73d-8cfbe105e4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Kan je aangifte doen tegen een agent?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwezen naar een klachtenprocedure in plaats van dat hun aangifte wordt opgenomen? Zo ja, wordt deze procedure duidelijk gecommu- niceerd aan burgers? 1 BNN-VARA, 1 maart 2025, Kan je aangifte doen tegen een agent? (https://www.bnnvara.nl/ artikelen/kan-je-aangifte-doen-tegen-een-agent-radio-boos). ah-tk-20242025-1735 ISSN 0921 - 7398 ’s-Gravenhage 2025 Tweede Kamer, vergaderjaar 2024–2025,\n",
            "\n",
            "5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwezen naar een klachtenprocedure in plaats van dat hun aangifte wordt opgenomen? Zo ja, wordt deze procedure duidelijk gecommu- niceerd aan burgers? 1 BNN-VARA, 1 maart 2025, Kan je aangifte doen tegen een agent? (https://www.bnnvara.nl/ artikelen/kan-je-aangifte-doen-tegen-een-agent-radio-boos). ah-tk-20242025-1735 ISSN 0921 - 7398 ’s-Gravenhage 2025 Tweede Kamer, vergaderjaar 2024–2025,\n",
            "\n",
            "5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwezen naar een klachtenprocedure in plaats van dat hun aangifte wordt opgenomen? Zo ja, wordt deze procedure duidelijk gecommu- niceerd aan burgers? 1 BNN-VARA, 1 maart 2025, Kan je aangifte doen tegen een agent? (https://www.bnnvara.nl/ artikelen/kan-je-aangifte-doen-tegen-een-agent-radio-boos). ah-tk-20242025-1735 ISSN 0921 - 7398 ’s-Gravenhage 2025 Tweede Kamer, vergaderjaar 2024–2025,\n",
            "\n",
            "Question: Kan je aangifte doen tegen een agent?\n",
            "Helpful Answer: 1 BNN-VARA, 1 maart 2025, Kan je aangifte doen tegen een agent? (https://www.bnnvara.nl/ artikelen/kan-je-aangifte-doen-tegen-een-agent-radio-boos). ah-tk-20242025-1735 ISSN 0921 - 7398 ’s-Gravenhage 2025 Tweede Kamer, vergaderjaar 2024–2025,\n",
            "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
            "Source Documents:\n",
            "- 5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwe... (from Uitstel beantwoording vragen van het lid Mutluer over het doen van aangifte tegen een politieagent.pdf)\n",
            "- 5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwe... (from Uitstel beantwoording vragen van het lid Mutluer over het doen van aangifte tegen een politieagent.pdf)\n",
            "- 5 Is het waar dat burgers die aangifte willen doen tegen een politiemedewerker vaak worden doorverwe... (from Uitstel beantwoording vragen van het lid Mutluer over het doen van aangifte tegen een politieagent.pdf)\n"
          ]
        }
      ],
      "source": [
        "example_query = \"Kan je aangifte doen tegen een agent?\"\n",
        "run_example_query(rag_chain, example_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lOeEzi5iplF",
        "outputId": "3759896a-58c0-499c-cf91-c7d01749d291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Wat zijn de beperkingsrechten van burgers over hun gezondheidsgegevens volgens de European Health Data Space-Verordening (EHDS), en hoe plant de Nederlandse minister van VWS deze te implementeren in nationale wetgeving?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European Health Data Space-Verordening (EHDS) draagt bij aan betere zorg door betere databeschikbaarheid. Die elektronische uitwisseling is essentieel, en daarbij ook de zeggenschap die burgers over hun zorg- en persoonsgegevens kunnen uitoefenen. De EHDS geeft burgers een aantal rechten waarmee zij die zeggenschap kunnen vormgeven. Heel belangrijk daarbij zijn beperkingsrechten, zoals de ‘opt-out’. In deze\n",
            "\n",
            "brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European Health Data Space-Verordening (EHDS) draagt bij aan betere zorg door betere databeschikbaarheid. Die elektronische uitwisseling is essentieel, en daarbij ook de zeggenschap die burgers over hun zorg- en persoonsgegevens kunnen uitoefenen. De EHDS geeft burgers een aantal rechten waarmee zij die zeggenschap kunnen vormgeven. Heel belangrijk daarbij zijn beperkingsrechten, zoals de ‘opt-out’. In deze\n",
            "\n",
            "brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European Health Data Space-Verordening (EHDS) draagt bij aan betere zorg door betere databeschikbaarheid. Die elektronische uitwisseling is essentieel, en daarbij ook de zeggenschap die burgers over hun zorg- en persoonsgegevens kunnen uitoefenen. De EHDS geeft burgers een aantal rechten waarmee zij die zeggenschap kunnen vormgeven. Heel belangrijk daarbij zijn beperkingsrechten, zoals de ‘opt-out’. In deze\n",
            "\n",
            "Question: Wat zijn de beperkingsrechten van burgers over hun gezondheidsgegevens volgens de European Health Data Space-Verordening (EHDS), en hoe plant de Nederlandse minister van VWS deze te implementeren in nationale wetgeving?\n",
            "Helpful Answer: De beperkingsrechten van burgers over hun gezondheidsgegevens volgens de European Health Data Space-Verordening (EHDS) zijn beperkingsrechten, zoals de 'opt-out'. In deze\n",
            "\n",
            "brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European Health Data Space-Verordening (EHDS) draagt bij aan betere zorg door betere databeschikbaarheid. Die elektronische uitwisseling is essentieel, en daarbij ook de zeggenschap die burgers over hun zorg- en persoonsgegevens kunnen uitoefenen. De EHDS geeft burgers een aantal rechten waarmee zij die zeggenschap kunnen vormgeven. Heel belangrijk daarbij zijn beperkingsrechten, zoals de ‘opt-out’. In deze\n",
            "Source Documents:\n",
            "- brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European He... (from Opt-out EHDS en andere toezeggingen.docx)\n",
            "- brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European He... (from Opt-out EHDS en andere toezeggingen.docx)\n",
            "- brief ‘agenda van databeschikbaarheid in de zorg’ waarmee ik u hierover informeerde.  De European He... (from Opt-out EHDS en andere toezeggingen.docx)\n"
          ]
        }
      ],
      "source": [
        "example_query = \"Wat zijn de beperkingsrechten van burgers over hun gezondheidsgegevens volgens de European Health Data Space-Verordening (EHDS), en hoe plant de Nederlandse minister van VWS deze te implementeren in nationale wetgeving?\"\n",
        "run_example_query(rag_chain, example_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzY7y9OFC-7_",
        "outputId": "e8abaa47-e7a3-422d-8da5-cfb845d25539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Wat is het opt-out-recht in de EHDS en hoe wil de Nederlandse minister van VWS dit toepassen?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit kan bespreek ik hieronder. Beperkingsrechten bij primair gebruik De EHDS biedt burgers twee beperkingsrechten: het recht op een opt-out en het recht op toegangsbeperking.  Recht op opt-out wordt in nationale wetgeving neergelegd Of EU-lidstaten het recht op opt-out voor hun eigen burgers mogelijk willen maken, mogen ze zelf bepalen. Doen ze dit, dan moet dit recht in nationale wetgeving worden\n",
            "\n",
            "de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit kan bespreek ik hieronder. Beperkingsrechten bij primair gebruik De EHDS biedt burgers twee beperkingsrechten: het recht op een opt-out en het recht op toegangsbeperking.  Recht op opt-out wordt in nationale wetgeving neergelegd Of EU-lidstaten het recht op opt-out voor hun eigen burgers mogelijk willen maken, mogen ze zelf bepalen. Doen ze dit, dan moet dit recht in nationale wetgeving worden\n",
            "\n",
            "de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit kan bespreek ik hieronder. Beperkingsrechten bij primair gebruik De EHDS biedt burgers twee beperkingsrechten: het recht op een opt-out en het recht op toegangsbeperking.  Recht op opt-out wordt in nationale wetgeving neergelegd Of EU-lidstaten het recht op opt-out voor hun eigen burgers mogelijk willen maken, mogen ze zelf bepalen. Doen ze dit, dan moet dit recht in nationale wetgeving worden\n",
            "\n",
            "Question: Wat is het opt-out-recht in de EHDS en hoe wil de Nederlandse minister van VWS dit toepassen?\n",
            "Helpful Answer: Recht op opt-out wordt in nationale wetgeving neergelegd Of EU-lidstaten het recht op opt-out voor hun eigen burgers mogelijk willen maken, mogen ze zelf bepalen. Doen ze dit, dan moet dit recht in nationale wetgeving worden\n",
            "Source Documents:\n",
            "- de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit ... (from Opt-out EHDS en andere toezeggingen.docx)\n",
            "- de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit ... (from Opt-out EHDS en andere toezeggingen.docx)\n",
            "- de EHDS, dan ook de mogelijkheid krijgen om zijn beperkingswensen te registeren. Op welke wijze dit ... (from Opt-out EHDS en andere toezeggingen.docx)\n"
          ]
        }
      ],
      "source": [
        "example_query = \"Wat is het opt-out-recht in de EHDS en hoe wil de Nederlandse minister van VWS dit toepassen?\"\n",
        "run_example_query(rag_chain, example_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya1SVFFtMzQk"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    del rag_chain\n",
        "    del vectorstore\n",
        "except NameError:\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "3_11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
